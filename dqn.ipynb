{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \n",
    "    def __init__(self, state_size, action_size):\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95    # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse',\n",
    "                      optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        \n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        \n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        \n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            \n",
    "            target = reward\n",
    "            \n",
    "            if not done:\n",
    "                \n",
    "                target = (reward + self.gamma *\n",
    "                          np.amax(self.model.predict(next_state)[0]))\n",
    "                \n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "            \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            \n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def load(self, name):\n",
    "        \n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        \n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0/5000, score: 10, e: 1.0\n",
      "episode: 1/5000, score: 17, e: 1.0\n",
      "episode: 2/5000, score: 16, e: 0.94\n",
      "episode: 3/5000, score: 16, e: 0.86\n",
      "episode: 4/5000, score: 16, e: 0.8\n",
      "episode: 5/5000, score: 20, e: 0.72\n",
      "episode: 6/5000, score: 53, e: 0.55\n",
      "episode: 7/5000, score: 50, e: 0.43\n",
      "episode: 8/5000, score: 32, e: 0.37\n",
      "episode: 9/5000, score: 51, e: 0.28\n",
      "episode: 10/5000, score: 36, e: 0.24\n",
      "episode: 11/5000, score: 51, e: 0.18\n",
      "episode: 12/5000, score: 80, e: 0.12\n",
      "episode: 13/5000, score: 87, e: 0.08\n",
      "episode: 14/5000, score: 127, e: 0.042\n",
      "episode: 15/5000, score: 113, e: 0.024\n",
      "episode: 16/5000, score: 116, e: 0.013\n",
      "episode: 17/5000, score: 130, e: 0.01\n",
      "episode: 18/5000, score: 126, e: 0.01\n",
      "episode: 19/5000, score: 94, e: 0.01\n",
      "episode: 20/5000, score: 174, e: 0.01\n",
      "episode: 21/5000, score: 156, e: 0.01\n",
      "episode: 22/5000, score: 229, e: 0.01\n",
      "episode: 23/5000, score: 171, e: 0.01\n",
      "episode: 24/5000, score: 139, e: 0.01\n",
      "episode: 25/5000, score: 241, e: 0.01\n",
      "episode: 26/5000, score: 148, e: 0.01\n",
      "episode: 27/5000, score: 138, e: 0.01\n",
      "episode: 28/5000, score: 147, e: 0.01\n",
      "episode: 29/5000, score: 141, e: 0.01\n",
      "episode: 30/5000, score: 145, e: 0.01\n",
      "episode: 31/5000, score: 24, e: 0.01\n",
      "episode: 32/5000, score: 22, e: 0.01\n",
      "episode: 33/5000, score: 120, e: 0.01\n",
      "episode: 34/5000, score: 22, e: 0.01\n",
      "episode: 35/5000, score: 114, e: 0.01\n",
      "episode: 36/5000, score: 142, e: 0.01\n",
      "episode: 37/5000, score: 162, e: 0.01\n",
      "episode: 38/5000, score: 134, e: 0.01\n",
      "episode: 39/5000, score: 151, e: 0.01\n",
      "episode: 40/5000, score: 154, e: 0.01\n",
      "episode: 41/5000, score: 146, e: 0.01\n",
      "episode: 42/5000, score: 157, e: 0.01\n",
      "episode: 43/5000, score: 138, e: 0.01\n",
      "episode: 44/5000, score: 144, e: 0.01\n",
      "episode: 45/5000, score: 187, e: 0.01\n",
      "episode: 46/5000, score: 154, e: 0.01\n",
      "episode: 47/5000, score: 54, e: 0.01\n",
      "episode: 48/5000, score: 150, e: 0.01\n",
      "episode: 49/5000, score: 177, e: 0.01\n",
      "episode: 50/5000, score: 51, e: 0.01\n",
      "episode: 51/5000, score: 141, e: 0.01\n",
      "episode: 52/5000, score: 201, e: 0.01\n",
      "episode: 53/5000, score: 430, e: 0.01\n",
      "episode: 54/5000, score: 195, e: 0.01\n",
      "episode: 55/5000, score: 155, e: 0.01\n",
      "episode: 56/5000, score: 325, e: 0.01\n",
      "episode: 57/5000, score: 66, e: 0.01\n",
      "episode: 58/5000, score: 115, e: 0.01\n",
      "episode: 59/5000, score: 158, e: 0.01\n",
      "episode: 60/5000, score: 88, e: 0.01\n",
      "episode: 61/5000, score: 147, e: 0.01\n",
      "episode: 62/5000, score: 102, e: 0.01\n",
      "episode: 63/5000, score: 169, e: 0.01\n",
      "episode: 64/5000, score: 112, e: 0.01\n",
      "episode: 65/5000, score: 223, e: 0.01\n",
      "episode: 66/5000, score: 152, e: 0.01\n",
      "episode: 67/5000, score: 97, e: 0.01\n",
      "episode: 68/5000, score: 145, e: 0.01\n",
      "episode: 69/5000, score: 170, e: 0.01\n",
      "episode: 70/5000, score: 219, e: 0.01\n",
      "episode: 71/5000, score: 156, e: 0.01\n",
      "episode: 72/5000, score: 162, e: 0.01\n",
      "episode: 73/5000, score: 210, e: 0.01\n",
      "episode: 74/5000, score: 208, e: 0.01\n",
      "episode: 75/5000, score: 163, e: 0.01\n",
      "episode: 76/5000, score: 165, e: 0.01\n",
      "episode: 77/5000, score: 156, e: 0.01\n",
      "episode: 78/5000, score: 132, e: 0.01\n",
      "episode: 79/5000, score: 157, e: 0.01\n",
      "episode: 80/5000, score: 13, e: 0.01\n",
      "episode: 81/5000, score: 116, e: 0.01\n",
      "episode: 82/5000, score: 33, e: 0.01\n",
      "episode: 83/5000, score: 25, e: 0.01\n",
      "episode: 84/5000, score: 8, e: 0.01\n",
      "episode: 85/5000, score: 381, e: 0.01\n",
      "episode: 86/5000, score: 148, e: 0.01\n",
      "episode: 87/5000, score: 187, e: 0.01\n",
      "episode: 88/5000, score: 130, e: 0.01\n",
      "episode: 89/5000, score: 173, e: 0.01\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "# agent.load(\"./save/cartpole-dqn.h5\")\n",
    "done = False\n",
    "batch_size = 32\n",
    "EPISODES = 5000\n",
    "\n",
    "for e in range(EPISODES):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    for time in range(500):\n",
    "        # env.render()\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        reward = reward if not done else -10\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            print(\"episode: {}/{}, score: {}, e: {:.2}\"\n",
    "                  .format(e, EPISODES, time, agent.epsilon))\n",
    "            break\n",
    "        if len(agent.memory) > batch_size:\n",
    "            agent.replay(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 10 timesteps\n",
      "Episode finished after 10 timesteps\n",
      "Episode finished after 10 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 10 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 10 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 10 timesteps\n",
      "Episode finished after 8 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 10 timesteps\n",
      "Episode finished after 10 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 8 timesteps\n",
      "Episode finished after 10 timesteps\n",
      "Episode finished after 10 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 10 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 10 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 8 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 8 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 8 timesteps\n",
      "Episode finished after 10 timesteps\n",
      "Episode finished after 10 timesteps\n",
      "Episode finished after 8 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 10 timesteps\n",
      "Episode finished after 10 timesteps\n",
      "Episode finished after 10 timesteps\n",
      "Episode finished after 10 timesteps\n",
      "Episode finished after 10 timesteps\n",
      "Episode finished after 10 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 12 timesteps\n",
      "Episode finished after 10 timesteps\n",
      "Episode finished after 10 timesteps\n",
      "Episode finished after 10 timesteps\n",
      "Episode finished after 10 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 10 timesteps\n",
      "Episode finished after 10 timesteps\n",
      "Episode finished after 8 timesteps\n",
      "Episode finished after 10 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 10 timesteps\n",
      "Episode finished after 10 timesteps\n",
      "Episode finished after 10 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 11 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 8 timesteps\n",
      "Episode finished after 10 timesteps\n",
      "Episode finished after 10 timesteps\n",
      "Episode finished after 10 timesteps\n",
      "Episode finished after 10 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 11 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 10 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 8 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 10 timesteps\n",
      "Episode finished after 8 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 8 timesteps\n",
      "Episode finished after 10 timesteps\n",
      "Episode finished after 10 timesteps\n",
      "Episode finished after 10 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 10 timesteps\n",
      "Episode finished after 11 timesteps\n",
      "Episode finished after 10 timesteps\n",
      "Episode finished after 10 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 10 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 12 timesteps\n",
      "Episode finished after 10 timesteps\n",
      "Episode finished after 10 timesteps\n",
      "Episode finished after 9 timesteps\n",
      "Episode finished after 9 timesteps\n"
     ]
    }
   ],
   "source": [
    "for i_episode in range(100):\n",
    "    \n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    \n",
    "    for t in range(100):\n",
    "        \n",
    "        env.render()\n",
    "#         print(state)\n",
    "        action = agent.act(state)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "football",
   "language": "python",
   "name": "football"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
